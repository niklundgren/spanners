### Quantum Espresso 

# Notes on various routines, install procedures, etc.



### IBRAV (Index of BRAVais lattice) - quantum espresso's routine for defining cell vectors

# 2 - fcc

a = celldm(1)
[ -a/2       0          a/2]
[   0       a/2         a/2]
[ -a/2      a/2          0 ]

# 4 - hexagonal / trigonal P boxes

a=celldm(1)
c=celldm(3)
[ a     0            0]
[-a/2   a*(sr3.2)    0]
[ 0     0          a*c]


### Running ph.x 

# Total number of process to be used on ph.x is
# ni x nk(scf)
# where ni is the number of images to parallelize
# and nk(scf) is the number of k-points parallelized in the pw.x run

1. mpirun -np A pw.x -nk B pw.x < scf.in

2. mpirun -np (AxC) ph.x -ni C -nk B < ph.in

To give you a bit of an idea, the -ni flag makes ph.x store the dynamical matrices
in a series of _ph<N> folders, where N is the image number.
They're independent calculations, so it can be parallelized easily, but when it's done
no one ever coded up the way to collect all the info so you need to do it manually.

To do that, collect all the dynmat.K1.K2.xml found in <outdir>/_ph*/pwscf.phsave/
(K1 = q-point, K2 = representation) into the <outdir>/_ph0/pwscf.phsave/ folder
That's the standard names for the directories in v7.2, it may be different 
if you use arguments for output folders.

3. mpirun -np A ph.x -nk B < restart_ph.in
    <--- that's a restart, which compiles the dynamical matrices
    <--- from the individual image calculations

To my ph.in file I usually add these arguments
"""
start_q=X
last_q=X
recover=.true.
"""
and just iterate over the q-points that didn't get diagonalized. Davide says there's a way to do
it where it should auto-detect, but it seems to take forever. Versus this
is basically instantaneous.


# Parallelization

N processes total
nk = k points
n3 = third dimension of fft grid
M = Khon Sham states

n3 <= N/nk
&
n3 should be integer multiple of 

Let's imagine you want to use 32 proccesses
N = 32
Using a 4x4x4 k-pt grid gives us a total of 64 k-points so nk<=64
Let's assume you use nk=4
n3 <= N/nk (32/4) = 8
No linear algebra parallelization unless M is huge (nd or openmp)
